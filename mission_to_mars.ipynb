{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using chromediver extension\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting my url variable \n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launching my browser using the chromedriver extension \n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting my html and reading it with beautiful soup \n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering my results down to the titles  based on the HTML class identified \n",
    "results = soup.body.find_all('div', class_='content_title')\n",
    "news_titles = []\n",
    "#looping through my results \n",
    "for result in results:\n",
    "    try:\n",
    "        #geting the text out of my result  \n",
    "        title = result.text\n",
    "        #print(title)\n",
    "        news_titles.append(title)\n",
    "    except AttributeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering my results down to the paragraphs  based on the HTML class identified \n",
    "results = soup.find_all('div', class_=\"article_teaser_body\")\n",
    "news_paragraphs = []\n",
    "for result in results:\n",
    "    try:\n",
    "        #retriving the text for paragraphs\n",
    "        paragraphs = result.text\n",
    "        #print(paragraphs)\n",
    "        news_paragraphs.append(paragraphs)\n",
    "    \n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest news title is \"Mars Helicopter Attached to NASA's Perseverance Rover\"\n",
      "The latest news paragragh is \"The team also fueled the rover's sky crane to get ready for this summer's history-making launch.\"\n"
     ]
    }
   ],
   "source": [
    "#printing the latest news from my url\n",
    "print(f'The latest news title is \"{news_titles[1]}\"')\n",
    "print(f'The latest news paragragh is \"{news_paragraphs[0]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting my url variable \n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launching my browser using the chromedriver extension \n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting my html and reading it with beautiful soup \n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering my results down to the image URL  based on the HTML class identified \n",
    "results = soup.body.find_all('a', class_='fancybox')\n",
    "all_images = []\n",
    "for result in results:\n",
    "    try:\n",
    "        #retriving the links tot he images\n",
    "        images = result.get('data-fancybox-href')\n",
    "        image_url = \"https://www.jpl.nasa.gov\" + str(images)\n",
    "        #print(image_url)\n",
    "        all_images.append(image_url)\n",
    "   \n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heres the link to the featured image  \"https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA18048_ip.jpg\"\n"
     ]
    }
   ],
   "source": [
    "#printing out the latest image URL from my list \n",
    "print(f'Heres the link to the featured image  \"{all_images[0]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "#launching my browser using the chromedriver extension \n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#filtering my result from the html page\n",
    "results = soup.body.find_all('div', class_='description')\n",
    "#Creating a list for my extracted results\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "#looping through my webpages\n",
    "for result in results:\n",
    "    page_link = result.find('a')['href']\n",
    "    url = \"https://astrogeology.usgs.gov\" + str(page_link)\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #filtering my result from the html page\n",
    "    res_url = soup.body.find_all('h2', class_='title')\n",
    "    title = res_url[0].text\n",
    "    \n",
    "    #filtering my result from the html page\n",
    "    res_url = soup.body.find_all('img', class_='wide-image')\n",
    "    image = res_url[0].get('src')\n",
    "    image_url = \"https://astrogeology.usgs.gov\" + str(image)\n",
    "    \n",
    "    #storing my results in a list \n",
    "    hemisphere_image_urls.append({\"Title\": title, \"Image_url\": image_url})\n",
    "\n",
    "# print(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Cerberus Hemisphere Enhanced', 'Image_url': 'https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg'}\n",
      "{'Title': 'Schiaparelli Hemisphere Enhanced', 'Image_url': 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg'}\n",
      "{'Title': 'Syrtis Major Hemisphere Enhanced', 'Image_url': 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg'}\n",
      "{'Title': 'Valles Marineris Hemisphere Enhanced', 'Image_url': 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg'}\n"
     ]
    }
   ],
   "source": [
    "#here are my results tile and image urls stored in  a python dictionary\n",
    "for result in hemisphere_image_urls:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pythondata]",
   "language": "python",
   "name": "conda-env-Pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
